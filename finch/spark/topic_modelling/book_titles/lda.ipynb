{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lda.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"EJNFOgo8NuK5","colab_type":"code","outputId":"645e6400-2fb4-4849-bceb-7cd622070fd9","executionInfo":{"status":"ok","timestamp":1566540049790,"user_tz":-480,"elapsed":44146,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCTG5IRIY_e34bmy4oR2FFkJW6Hlvr9nFF4nYPVlA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["!apt-get install openjdk-8-jdk-headless > /dev/null\n","!wget http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\n","!tar xf spark-2.4.3-bin-hadoop2.7.tgz\n","!pip install findspark"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2019-08-23 06:00:11--  http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\n","Resolving mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)... 101.6.8.193, 2402:f000:1:408:8100::1\n","Connecting to mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)|101.6.8.193|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 229988313 (219M) [application/octet-stream]\n","Saving to: ‘spark-2.4.3-bin-hadoop2.7.tgz’\n","\n","spark-2.4.3-bin-had 100%[===================>] 219.33M  9.84MB/s    in 28s     \n","\n","2019-08-23 06:00:39 (7.95 MB/s) - ‘spark-2.4.3-bin-hadoop2.7.tgz’ saved [229988313/229988313]\n","\n","Requirement already satisfied: findspark in /usr/local/lib/python3.6/dist-packages (1.3.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_owq4Fy85YUL","colab_type":"code","outputId":"f232483a-2b29-42ab-a9cb-2fb6909d45df","executionInfo":{"status":"ok","timestamp":1566540049793,"user_tz":-480,"elapsed":44092,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCTG5IRIY_e34bmy4oR2FFkJW6Hlvr9nFF4nYPVlA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\"\"\"\n","We are running these lines because we are operating on Google Colab\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.3-bin-hadoop2.7\"\n","os.chdir('/content/gdrive/My Drive/finch/spark/topic_modelling/book_titles')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v8CmbbII_qvZ","colab_type":"code","colab":{}},"source":["import nltk\n","from nltk.corpus import stopwords\n","\n","import findspark\n","findspark.init()\n","\n","from pyspark import SparkContext\n","from pyspark.sql import SparkSession, Row\n","from pyspark.sql.types import StringType, ArrayType\n","from pyspark.ml.feature import CountVectorizer, IDF\n","from pyspark.ml.clustering import LDA"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiC5WoSYo7qR","colab_type":"code","colab":{}},"source":["N_TOPICS = 10\n","MAX_TERMS = 5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8uJ8VlTR_t5h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"ffecf079-c21a-4b8b-c759-4d2da9d78fed","executionInfo":{"status":"ok","timestamp":1566540074083,"user_tz":-480,"elapsed":68317,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCTG5IRIY_e34bmy4oR2FFkJW6Hlvr9nFF4nYPVlA=s64","userId":"01997730851420384589"}}},"source":["nltk.download('stopwords')\n","stopwords = set(stopwords.words('english')).union({\n","    'introduction', 'edition', 'series', 'application',\n","    'approach', 'card', 'access', 'package', 'plus', 'etext',\n","    'brief', 'vol', 'fundamental', 'guide', 'essential', 'printed',\n","    'third', 'second', 'fourth'})\n","\n","sc = SparkContext('local', 'nlp')\n","lines = sc.textFile('./all_book_titles.txt')\n","lines = lines \\\n","    .map(lambda line: line.strip().lower()) \\\n","    .map(lambda line: line.split()) \\\n","    .map(lambda words: [w for w in words if w.isalpha()]) \\\n","    .map(lambda words: [w for w in words if len(w) > 3]) \\\n","    .map(lambda words: [w for w in words if w not in stopwords]) \\\n","    .zipWithIndex()\n","\n","sess = SparkSession.builder.appName('nlp').getOrCreate()\n","df = sess.createDataFrame(lines, ['words', 'idx'])\n","\n","cv = CountVectorizer(inputCol='words',\n","                     outputCol='tf')\n","cv = cv.fit(df)\n","df = cv.transform(df)\n","df = IDF(inputCol='tf',\n","         outputCol='tfidf').fit(df).transform(df)\n","\n","lda = LDA(k=N_TOPICS,\n","          featuresCol='tfidf',\n","          optimizer='em').fit(df)\n","\n","for i, indices in enumerate(lda.describeTopics(MAX_TERMS).toPandas().termIndices):\n","    print('Topic %d:'%(i+1), ' '.join([cv.vocabulary[idx] for idx in indices]))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Topic 1: probability statistics finance management essentials\n","Topic 2: mechanics physical differential equations problems\n","Topic 3: computer design data physics engineers\n","Topic 4: world database asian human molecular\n","Topic 5: handbook evolution medical general medicine\n","Topic 6: political structures theater integrated programming\n","Topic 7: psychology pharmacology insurance thermodynamics biochemistry\n","Topic 8: philosophy actuarial reader solutions readings\n","Topic 9: language natural machine international systems\n","Topic 10: history science human american volume\n"],"name":"stdout"}]}]}