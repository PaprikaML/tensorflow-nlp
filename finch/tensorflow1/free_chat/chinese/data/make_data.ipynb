{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"make_data.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"2AmzAFs-s22t","colab_type":"code","colab":{}},"source":["\"\"\"\n","We use following lines because we are running on Google Colab\n","If you are running notebook on a local computer, you don't need this cell\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir('/content/gdrive/My Drive/finch/tensorflow1/free_chat/chinese/data')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQqlO5uwtqkt","colab_type":"code","outputId":"30cc1ac0-c23f-4b6d-f94b-4bd079ffb5a4","executionInfo":{"status":"ok","timestamp":1574329345326,"user_tz":-480,"elapsed":236263,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCTG5IRIY_e34bmy4oR2FFkJW6Hlvr9nFF4nYPVlA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":715}},"source":["%tensorflow_version 1.x\n","!pip install texar"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting texar\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/57/1b8af36cca480965e3d50ec218017f1c3c8732e07aa8d8d802ef4ecd9561/texar-0.2.4.tar.gz (290kB)\n","\u001b[K     |████████████████████████████████| 296kB 5.1MB/s \n","\u001b[?25hCollecting regex>=2018.01.10\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n","\u001b[K     |████████████████████████████████| 645kB 45.1MB/s \n","\u001b[?25hCollecting numpy<1.17.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/41283370f942f647422581eed16df4b653a744a3e9d5cfbb9aee0440f6eb/numpy-1.16.5-cp36-cp36m-manylinux1_x86_64.whl (17.4MB)\n","\u001b[K     |████████████████████████████████| 17.4MB 127kB/s \n","\u001b[?25hRequirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.6/dist-packages (from texar) (1.0.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from texar) (3.13)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from texar) (2.21.0)\n","Collecting funcsigs>=1.0.2\n","  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n","Collecting sentencepiece>=0.1.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 41.5MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from texar) (19.2)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->texar) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->texar) (2019.9.11)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->texar) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->texar) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->texar) (2.4.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->texar) (1.12.0)\n","Building wheels for collected packages: texar\n","  Building wheel for texar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for texar: filename=texar-0.2.4-cp36-none-any.whl size=444369 sha256=881bec5b973437fa5bf5863d59af52ef2016ed7a8370e1a90a41cf3025bcfd41\n","  Stored in directory: /root/.cache/pip/wheels/d3/10/64/40ab4b8563fe393750ada8484c27da019b15b0ff51ffcbaf95\n","Successfully built texar\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: regex, numpy, funcsigs, sentencepiece, texar\n","  Found existing installation: numpy 1.17.4\n","    Uninstalling numpy-1.17.4:\n","      Successfully uninstalled numpy-1.17.4\n","Successfully installed funcsigs-1.0.2 numpy-1.16.5 regex-2019.11.1 sentencepiece-0.1.83 texar-0.2.4\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"qFUF6eDF2Uhd","colab_type":"code","colab":{}},"source":["from collections import Counter\n","import json\n","import random\n","import re\n","import texar.tf as tx"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aMmVyHu_tDzU","colab_type":"text"},"source":["Online Collection Part"]},{"cell_type":"code","metadata":{"id":"65Z9kT3ks_7K","colab_type":"code","colab":{}},"source":["char_counter = Counter()\n","\n","qa_pairs = set() # to remove duplicate dialogue pairs\n","\n","with open('../data/raw_data/xiaohuangji50w_nofenci.conv') as f:\n","  xiaohuangji = f.read().split('E')\n","\n","with open('../data/raw_data/stc-3_emotion_train.json') as f:\n","  \n","  for turn in xiaohuangji:\n","    li = turn.strip().split('M ')\n","    if len(li) == 3:\n","      q, a = li[1:]\n","      q = q.strip()\n","      a = a.strip()\n","      if a == '= =' or a == '=。=':\n","        continue\n","      \n","      qa_pairs.add((q, a))\n","      \n","      char_counter.update(list(q))\n","      char_counter.update(list(a))\n","  \n","  for li in json.load(f):\n","    if len(li) > 2:\n","      continue\n","    else:\n","      (q, _), (a, _) = li\n","      q, a = ''.join(q.split()), ''.join(a.split())\n","      \n","      qa_pairs.add((q, a))\n","      \n","      char_counter.update(list(q))\n","      char_counter.update(list(a))\n","\n","with open('../data/full.txt', 'w') as f_out:\n","  for q, a in qa_pairs:\n","    f_out.write(q+'|'+a+'\\n')\n","  \n","chars = ['<pad>', '<start>', '<end>'] + [char for char, freq in char_counter.most_common() if freq >= 5]\n","with open('../vocab/char.txt', 'w') as f:\n","  for c in chars:\n","    f.write(c+'\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VmboW7-duVmm","colab_type":"text"},"source":["Reduce Online Collection"]},{"cell_type":"code","metadata":{"id":"5AfvJMRluZ-F","colab_type":"code","colab":{}},"source":["with_scores = []\n","with open('full.txt') as f:\n","  for line in f:\n","    line = line.rstrip()\n","    sp = line.split('|')\n","    if len(sp) == 2:\n","      source, target = sp\n","    else:\n","      continue\n","    if source == target:\n","      continue\n","    if '笑话' in source:\n","      continue\n","    if '傻逼' in target:\n","      continue\n","    try:\n","      score = tx.evals.sentence_bleu(references=[list(source)], hypothesis=list(target), max_order=2)\n","    except:\n","      continue\n","    if score > 70.:\n","      continue\n","    if score == 0.:\n","      continue\n","    with_scores.append((source, target, score))\n","\n","with_scores.sort(key=lambda x: x[2])\n","with open('bleu_score.txt', 'w') as f_bleu, open('reduced.txt', 'w') as f_reduced:\n","  for triple in reversed(with_scores):\n","    source, target, score = triple\n","    f_bleu.write(source+'|'+target+'|'+str(score)+'\\n')\n","    f_reduced.write(source+'|'+target+'\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QSK-xydCtIjl","colab_type":"text"},"source":["Human Labelled Part"]},{"cell_type":"code","metadata":{"id":"ok-FkzlTtH8E","colab_type":"code","colab":{}},"source":["with open('../data/raw_data/results1017.txt') as f, open('../data/core.txt', 'w') as f_out:\n","  for i, line in enumerate(f):\n","    if i == 0:\n","      continue\n","    line = line.rstrip()\n","    sp = line.split('\\t')\n","    \n","    if len(sp) > 2:\n","      f_out.write(sp[0]+'|'+sp[2]+'\\n')\n","    else:\n","      f_out.write(sp[0]+'|'+re.sub(r'\\(.*\\)\\s', '', sp[1])+'\\n')"],"execution_count":0,"outputs":[]}]}