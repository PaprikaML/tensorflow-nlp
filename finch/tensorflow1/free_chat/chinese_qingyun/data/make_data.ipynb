{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"make_data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMQF+QPjMZ0Z2mXAJwlz/Hf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"D9BYUhyoWn_r","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir('/content/gdrive/My Drive/finch/tensorflow1/free_chat/chinese_qingyun/data')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvnDlghfbPS2","colab_type":"code","colab":{}},"source":["from collections import Counter\n","from pathlib import Path\n","\n","import numpy as np\n","import random\n","import re"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TGCf1kNnWx_O","colab_type":"code","outputId":"01ca9dbf-982e-4a9f-c31a-65d9f1ad4616","executionInfo":{"status":"ok","timestamp":1586759779415,"user_tz":-480,"elapsed":96429,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["Path('../vocab').mkdir(exist_ok=True)\n","char_counter = Counter()\n","src_lens, tgt_lens = [], []\n","\n","with open('./raw_data.csv') as f, open('./train.txt', 'w') as f_tr, open('./test.txt', 'w') as f_te:\n","  for line in f:\n","    line = line.rstrip().lower()\n","    line = re.sub('{.*}', ' ', line)\n","    line = line.replace('★', ' ')\n","    line = re.sub('\\s+', ' ', line)\n","    if ('我的粉丝也不是' in line) or ('qq' in line) or ('菲菲' in line) or ('飲水得喇' in line):\n","      continue\n","    src, tgt = line.split(' | ')\n","    src = src.strip()\n","    tgt = tgt.strip()\n","    if len(src) > 0 and len(tgt) > 0:\n","      if random.random() < 0.03:\n","        f_te.write(src+'<SEP>'+tgt+'\\n')\n","      else:\n","        f_tr.write(src+'<SEP>'+tgt+'\\n')\n","      char_counter.update(list(src))\n","      char_counter.update(list(tgt))\n","      src_lens.append(len(src))\n","      tgt_lens.append(len(tgt))\n","\n","print('Source Average Length', sum(src_lens)/len(src_lens))\n","print('Target Average Length', sum(tgt_lens)/len(tgt_lens))\n","\n","chars = ['<pad>', '<start>', '<end>'] + [char for char, freq in char_counter.most_common() if freq >= 5]\n","print(len(chars), 'Chars')\n","with open('../vocab/char.txt', 'w') as f:\n","  for c in chars:\n","    f.write(c+'\\n')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Source Average Length 6.726910849536641\n","Target Average Length 10.60665363797653\n","3860 Chars\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-JsCMhBmcu6_","colab_type":"code","outputId":"fbbbd7ae-8570-485e-ba66-79a192195b32","executionInfo":{"status":"ok","timestamp":1586759854622,"user_tz":-480,"elapsed":171627,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":436}},"source":["char2idx = {}\n","with open('../vocab/char.txt') as f:\n","  for i, line in enumerate(f):\n","    line = line.rstrip('\\n')\n","    char2idx[line] = i\n","\n","embedding = np.zeros((len(char2idx)+1, 300)) # + 1 for unknown word\n","\n","with open('../vocab/cc.zh.300.vec') as f:\n","  count = 0\n","  for i, line in enumerate(f):\n","    if i == 0:\n","      continue\n","    if i % 100000 == 0:\n","      print('- At line {}'.format(i))\n","    line = line.rstrip()\n","    sp = line.split(' ')\n","    word, vec = sp[0], sp[1:]\n","    if word in char2idx:\n","      count += 1\n","      embedding[char2idx[word]] = np.asarray(vec, dtype='float32')\n","      \n","print(\"[%d / %d] characters have found pre-trained values\"%(count, len(char2idx)))\n","np.save('../vocab/char.npy', embedding)\n","print('Saved ../vocab/char.npy')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["- At line 100000\n","- At line 200000\n","- At line 300000\n","- At line 400000\n","- At line 500000\n","- At line 600000\n","- At line 700000\n","- At line 800000\n","- At line 900000\n","- At line 1000000\n","- At line 1100000\n","- At line 1200000\n","- At line 1300000\n","- At line 1400000\n","- At line 1500000\n","- At line 1600000\n","- At line 1700000\n","- At line 1800000\n","- At line 1900000\n","- At line 2000000\n","[3804 / 3860] characters have found pre-trained values\n","Saved ../vocab/char.npy\n"],"name":"stdout"}]}]}